---
layout: post
title: "Использование Docker для кросскомпиляции проектов"
permalink: docker-cpp
tags: C++ Docker GitLab
---

<img src='/assets/2017-05-15-docker-c++/cplus-docker.jpg' style="width: 720px;">
Docker как незаметный и незаменимый центральный элемент в разработке, continuous integration под множество разных целевых платформ.
Как не проходить квест по настройке окружения каждый раз.

---

*Важно*: все ниже сказаное описывает мой опыт работы в основном с embedded linux C/C++ проектами. Просьба помнить это во время чтения.

## Содержание
- [Проблема](#problem)
- [Постановка задачи](#formulation)
- [Решение](#solution)
    - [Docker](#docker)
    - [Docker Registry](#registry)
    - [Контейнеры и Continuous Integration(на базе GitLab)](#docker_and_ci)
    - [Минусы и их решения](#docker_minuses)
- [Заключение](#resume)

## Проблема    {#problem}
Сколько времени должен потратить новый сотрудник на то, чтобы собрать не знакомый ему проект? Обычно это суммарное время, необходимое на следующие действия:

 - выкачать/склонировать репозиторий проекта;
 - установить/настроить средства разработки(тулчейн, IDE, библиотеки);
 - разложить все по правильным путям, сконфигурировать make/cmake/qmake-файл/project_build.sh для сборки;
 - запустить саму сборку;
 - запустить/задеплоить приложение/сервис;

По дороге может выясниться, что где-то чего-то не хватает в зависимостях, неправильно прописан путь к либе/утилите/тулчейну, не та версия библиотеки установлена. И пока решишь все эти мелкие проблемы - пройдет значительный кусок времени. Плюс еще по любому будут привлекаться сотрудники, которые уже давно работают с проектом.

Обычно для того что бы каждый раз не набивать шишки - пишется некий README, где прописываются *точные* инструкции, по выполнению которых можно добиться воспроизводимого результата. За пару человек эти инструкции отлаживаются и наступает счастье.

В идеале хочется чтобы сборка проекта выглядела подобным образом:  

{% highlight bash %}
    install all required software, SDKs, libs
    git clone git@project.git
    cd project_dir
    make/cmake/build_project.sh
{% endhighlight %}

Дальнейший шаг - инструкции из README автоматизируются скриптом.

Какие варианты решений я встречал/проходил:

 - каждый сам себе все настраивает: долго, не воспроизводимо(проблема ["у меня на компьютере все работает"](http://lurkmore.to/%D0%A3%D0%9C%D0%92%D0%A0), так как у каждого разработчика разное окружение), подвержено ошибкам конфигураций. При выпуске новой версии SDK - прохождение квеста каждым разработчиком заново. Автоматизация сборки всего shell скриптом может частично решить проблему;
 - единое окружение у всех разработчиков, варианты обеспечения:
    - единожды настроенная виртуальная машина билмастером, все ее себе копируют и работают локально. Новое SDK/библиотека/что-то пропатчено/какое-то важное изменение - новая виртуалка. Тяжело, много места, неудобно;
    - настроенные сервера, у каждого есть свой аккаунт, все туда логинятся по ssh, у каждого своя копия исходников, которая примонтирована на локальную машину разработчика по samba/nfs/sshfs/whatever, и все собираются в уже готовом и настроенном окружении. Сеть/сервер упали - разработчики ковыряются в носу. Качество работы определяется качеством работы сети, в целом невысокая производительность работы IDE(парсинг проекта, подсветка), так как сеть в любом случае проигрывает в скорости локальному диску. Очень осторожно надо менять настройки окружения, ибо если накосячить случайно - зацепит всех, не классно;
    - [linux контейнеры](https://ru.wikipedia.org/wiki/Виртуализация_на_уровне_операционной_системы) - об этом и поговорим далее.

## Постановка задачи    {#formulation}

Представим следующую ситуацию:

 - есть n-ное количество целевых платформ(разные версии linux, gcc, разные процы: [ARM](https://ru.wikipedia.org/wiki/ARM_(%D0%B0%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D0%B0)), [MIPS](https://ru.wikipedia.org/wiki/MIPS_(%D0%B0%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D0%B0)), [SuperH](https://ru.wikipedia.org/wiki/SuperH), x86-64), под который проект собирается, соответственно разные тулчейны для кросскомпиляции;
 - под каждый конкретный процессор идет собственное SDK от чипмейкера. Есть различные версии одного и того же SDK;
 - в SDK находятся баги, их патчат(причем иногда это заметно "сверху", т.е. пользователям SDK, так как могут поменятся интерфейсы), соответственно SDK необходимо пересобрать, бинари/хидера расшарить между разработчиками;
 - то же самое касается еще ряда библиотек, которые работают поверх SDK, например, Qt, Chromium;
 - необходимо свести к минимуму время на развертывание окружения для разработки под конкретную платформу;
 - большинству разработчиков вообще не надо или не интересно заморачиваться настройкой/обновлением окружения, им бы сразу попасть в готовое, где можно писать уже непосредственно код;
 - весь зоопарк платформ надо как-то тестировать, желательно автоматически, хотя бы на предмет успешной компиляции кода проекта под все платформы и при этом не хочется постоянно перенастраивать CI для того, чтобы поддерживать окружения в актуальном состоянии;

Эволюционно решение предстало в виде контейнеризации окружения. Одна платформа - один контейнер со всем необходимым содержимым. Его легко скопировать себе на машину может каждый разработчик и тут же начать работать. Это несомненно проще, чем проходить квест по настройке/обновлению окружения в соответствии с README проекта.

Сразу же на поверхность выплыл еще один вопрос: распостранение контейнеров. Бегать с флешкой/ходить по сети куда-то - не классно, плюс человеческая натура такова, что даже если десять раз скомандовать в рабочем чатике: "обновляемся!", - момент действия будет отложен как можно дальше. Возможность централизированной раздачи контейнеров, да еще если это можно сделать "незаметно" для разработчика - это будет просто песня!

Из всех вариантов(LXC, [OpenVZ](https://openvz.org/), [Docker](https://www.docker.com/) и т.д.), которые были доступны на момент исследований, лишь Docker подошел целиком и полностью.

## Решение  {#solution}

### Docker  {#docker}

Цитата с [вики](https://ru.wikipedia.org/wiki/Docker):

    Docker — программное обеспечение для автоматизации развёртывания и управления приложениями в среде виртуализации на уровне операционной системы. Позволяет «упаковать» приложение со всем его окружением и зависимостями в контейнер, который может быть перенесён на любую Linux-систему с поддержкой cgroups в ядре, а также предоставляет среду по управлению контейнерами.

Как было до внедрения docker'a: были сервера, где располагались настроенные/собранные sdk/библиотеки. Все заинтересованные разработчики логинились туда по ssh, монтировали с сервера на свой локальный хост по nfs/samba/sshfs папки с проектами, открывали их в локальном любимом редакторе. На локальном хосте редактировали, на удаленном сервере - компилировали. Это работало: у всех единное окружение, настраивать его всем каждый раз не надо. Но медленно: проекты большие, IDE постоянно подвисает на сетевых операциях, на сервере всегда кто-то компилирует - ты ждешь пока скомпилятся твои изменения, в то время как твой хост с 4хядерным Intel Core i7 просто простаивает, печаль:(.

После введения контейнеров, как у нас в компании они используется:

 - каждый разработчик работает локально, не зависит от сети и ее лагов;
 - под одну платформу есть две "последние" версии docker-образа: latest и latest_debug. Первая версия - это почищенная вторая от временных файлов(обьектники, архивы, сырцы либ, .git-папки и все то что точно не потребуется для разработки приложений просто использующих sdk/библиотеки), соотв. latest_debug - это просто дамп всего того что нагенерировалось во время сборки sdk и библиотек;
 - разработчики приложений - используют готовое запакованное в latest docker-образ окружение для сборки проектов;
 - разработчики платформы - пилят и патчат саму программную "платформу": то что используется разработчиками приложений. Т.е. linux, SDK от чипмейкера, Qt, Webkit\Chromium. Для этого используют latest_debug версию контейнера. Не страшно накосячить и что-то не так сделать - в крайнем случае все просто потеряется и откатится к первоначальному состоянию. Потому можно смело эксперементировать, не боясь "сломать" достаточно сложную многоступенчатую сборку "мира", которая уже настроена. Опять же, удобно использовать latest_debug версию из-за того что уже есть все в собранном состоянии, при изменениях компилируется только дельта. Т.е. не приходится ждать пока собирется, например, весь Qt5 Framework(который еще надо и корректно сконфигурировать для кросскомпиляции!);
 - latest docker-образы используются на CI. Всегда есть платформы в состоянии активной разработки и те, которые сейчас на поддержке, "туда разработчики редко заходят". До введения CI часто возникали ситуации, когда билд был сломан для редкоразрабатываемой на данный момент платформы. И прежде чем приступить к работе, необходимо было в обязательном порядке фиксить ошибки. CI просто исключил такие ситуации - виновник <s>торжества</s> поломки сразу получает письмо от GitLab и бежит исправлять ошибки;
 - введен в строй приватный [docker registry](#registry) - сервер хранения образов - все заинтересованные получают образы только с этого сервера. Туда же выкладываются обновленые версии образов;

Картинка: как собирается docker образ.

Как запускается докер образ

Картинка: что происходит по push'у разработчика в репозиторий проекта(как CI работает и взаимодействует с docker registry)


### Docker Registry {#registry}

С офф. [сайта](https://docs.docker.com/registry/):

    The Registry is a stateless, highly scalable server side application that stores and lets you distribute Docker images. The Registry is open-source, under the permissive Apache license.
    
В общем-то все сказано:). Централизованная раздача docker-образов. Можно скидывать образы на какой-то ftp/web-сервер, облако. Тогда каждый разработчик сам сознательно должен скачать на свою машину, что-то вроде следующих действий:

    wget https://cloud.company.com/path/to/dockers/specific_img_version.tar
    docker load specific_img_version.tar
    rm -f specific_img_version.tar

Где specific_img_version.tar - название экспортированного docker образа.

А можно поднять свой Docker Registry, откуда сам docker клиент будет тянуть требуемый образ.

В общем случае мне кажется логичным использовать для этих целей [Docker Hub](hub.docker.com) - аналог Github в мире docker контейнеров. Там можно размещать неограниченное количество публичных образов и только один приватный. Большее количество приватных образов можно добавлять за отдельную плату. Если нет ничего секретного - заводим аккаунт, выкладываем образы, которые публичны и доступны всем. Можно купить подписку и размещать приватные образа, т.е. публично не доступные. Эти два варианты клевые тем, что сразу снимается вопрос поддержки, за нас это делают админы Docker Hub. В нашем случае это не подходило, потому подняли собственный docker registry. Он не такой крутой как Docker Hub. Нет например какого либо веб интефейса. "Общаться" с сервером можно через rest api. Например, вот как получить список репозиториев при помощи curl'a:
    
    curl -X bla bla bla

Сам сервер поставляется в виде docker-образа, как не странно, запускается следующим образом:

    docker run --rm -d --restart -p 5000:5000 --name registry   #insert exact command
    
После этого у вас есть настроенный и готовый для работы регистр.

*Важное* примечание: docker client по умолчанию работает через https. Если у вас нет/не планируется SSL для домена, на котором будет крутиться регистр, то каждом хосте, где будет запускаться docker client, необходимо [прописать](https://docs.docker.com/registry/insecure/#deploy-a-plain-http-registry) адрес "небезопасного" docker registry в /etc/docker/daemon.json:

    {
      "insecure-registries" : ["registry_ip_without_ssl.com:5000"]
    }

По умолчанию docker client всегда "смотрит" на Docker Hub. Для того что бы он мог пушить и пулить образы с других хранилищ, в имя образа всегда надо добавлять адрес docker registry. Пример имени, который используется у нас в компании:

    hub.company.com:5000/chipmaker_name/platform_name:latest

где hub.company.com:5000 - url и порт, на котором работает docker regsitry, если эта часть будет пустая docker client будет посылать запросы на Docker Hub.
Вся дальнейшая часть имени может быть абсолютно произвольной.

Отправить образ в хранилище:

    docker push registry_ip:registry_port/image_name:image_version

Получить образ из хранилища:

    docker pull registry_ip:registry_port/image_name:image_version

В общем-то это все команды, которые используются у нас в компании для взаимодействия между docker client и docker registry.


Схема работы такова:

 - на рабочем компьютере билдмастера собирается образ, который содержит все необходимое для разработки под конкретную платформу, и отправляется в регистр:
{% highlight bash %}
    docker push registry_ip:registry_port/image_name:image_version
{% endhighlight %}
 - все желающие забирают/пулят образ из регистра и работают в готовом окружении:
{% highlight bash %}
    docker pull registry_ip:registry_port/image_name:image_version
{% endhighlight %}


запуск:
{% highlight bash %}
    docker run -it --rm -v /path/to/project/sources:/path/of/sources/inside/container registry_ip:registry_port/image_name:image_version bash
{% endhighlight %}
В результате получаем обычную командную строку(в данном случае bash), в которой можно перейти в папку с проектом и вызвать make/cmake/build.sh
Можно запустить сразу билд, что бы ручками не ходить куда-то:

{% highlight bash %}
    docker run -it --rm -v /path/to/project/sources:/path/of/sources/inside/container registry_ip:registry_port/image_name:image_version bash -c "cd /path/of/sources/inside/container; make/cmake/build.sh;"
{% endhighlight %}
Команду выше можно использовать для интеграции с IDE: в каждой приличной IDE есть что-то вроде custom build steps, куда приведенную команду можно прописать.






Копипасты выводов:
{% highlight bash %}
alex@alex-pc:~$ docker images
REPOSITORY                                          TAG                    IMAGE ID            CREATED             SIZE
broadcom/platform351                                16.4_v0                e90ae3da554e        2 days ago          5.87 GB
hub.docker.company.com:5000/chipmaker/platform351   16.4_v3                e90ae3da554e        2 days ago          5.87 GB
hub.docker.company.com:5000/chipmaker/platform351   latest                 e90ae3da554e        2 days ago          5.87 GB
hub.docker.company.com:5000/chipmaker/platform324   latest                 b363901a6dda        2 days ago          5.18 GB
v43_hi310_4.4                                       latest                 a2c5423f0ccc        2 weeks ago         8.38 GB
v43_hi320_4.4                                       latest                 c3b9dbe0b582        2 weeks ago         7.9 GB
chipmaker/platform420                               base                   1f58ee5af66c        3 weeks ago         413 MB
hitool                                              1                      7846efda5be6        3 weeks ago         866 MB
chipmaker/platform420                               V100R005C00SPC040_v1   23e9f56aa5be        3 weeks ago         9.98 GB
ubuntu                                              14.04.5                302fa07d8117        2 months ago        188 MB
nate/dockviz                                        latest                 4ea294712533        6 months ago        7.33 MB
starefossen/github-pages                            41                     78ef097875de        17 months ago       859 MB
starefossen/github-pages                            40                     84156a8a47a9        18 months ago       849 MB
cmaohuang/firefox-java                              latest                 e31b29d62bab        21 months ago       635 MB
alex@alex-pc:~$ docker ps -a
CONTAINER ID      IMAGE                      COMMAND                  CREATED     STATUS       PORTS                                NAMES
9b9b5a7d3821    starefossen/github-pages:41 "jekyll serve --dr..."   2 hours ago  Up 2 hours   0.0.0.0:4000->4000/tcp, 4100/tcp     zealous_snyder
8e2801d70d9f    chipmaker/platform420:base  "bash"                   3 days ago   Up 2 days                                         Hi3798MV200_41
0e5890960206    chipmaker/platform420:base  "bash"                   2 weeks ago  Up 2 weeks                                        v43_310_1024M
ed2988615670    chipmaker/platform420:base  "--name v43_310_10..."   2 weeks ago  Created                                           romantic_murdock
8e6f24831527    v43_hi310_4.4               "bash"                   2 weeks ago  Up 2 weeks                                        qt_softfp
{% endhighlight %}

## Скрипт-обертка

 - один и тот же везде: и для разработчиков и для CI;
 - лежит в гите вместе с проектом;
 - решает следующие задачи:
    - нулевое вхождение: не надо изучать docker чтобы пользоваться;
    - "прокидывает" папку с проектом внутрь контейнера;
    - создает "на лету" пользователя аналогичного хостовому - нет проблем с правами на сгенерированные файлы(те, которые получились в результате компиляции);
    - настраивает корректную работу git внутри контейнера;
    - проверяет на наличие и удаляет "висящие" слои/образы, остановленные контейнеры - место на диске не пропадает впустую;
    - кого не устраивает дефолтная функциональность - используют докер напрямую с его километровыми командами:)

скрипт запуска:
{% highlight bash %}
#!/bin/bash

DOCKER_IMG_NAME="docker_image_name:latest"

set -e

docker pull ${DOCKER_IMG_NAME}

self_script_name="$(basename "$(test -L "$0" && readlink "$0" || echo "$0")")"

src_dir=${1?Usage: ${self_script_name} path_to_sources}
resolved_dir=$(readlink -f $src_dir)

echo "Will mount host path \"${resolved_dir}\" to container path \"/src\""

hint_msg=$(cat <<EOF
#################################################
#Some infomation tips                           #
#################################################
EOF
)
bash_cmd=$(cat <<EOF
echo '${hint_msg}' && bash
EOF
)

docker run -it --rm -v ${resolved_dir}:/src -e LOCAL_USER_ID=`id -u ${USER}` ${DOCKER_IMG_NAME} bash -c "${bash_cmd}"
{% endhighlight %}


## Автоматическая сборка образов

 - есть репозиторий, который содержит dockerfile'ы, bash-скрипты, конфиги для сборки docker-образа под ту или иную платформу;
 - есть репозиторий с патчами для SDK, структура приблизительно следующая:
 
    Patches_repo  
        |-->Platform1  
        |       |-->SDKv1  
        |       |     |-->01_patch_name.patch  
        |       |     |-->02_patch_name.patch  
        |       |     |-->.....  
        |       |     |-->nn_patch_name.patch  
        |       |-->SDKv1.5  
        |             |-->01_patch_name.patch  
        |             |-->02_patch_name.patch  
        |             |-->.....  
        |             |-->nn_patch_name.patch  
        |-->Platform2  
        |       |-->SDKv1  
        |       |     |-->...  
        |       |-->SDKv1.7  
        |             |-->...  
        |-->....  
        |-->PlatformN  
                |-->...  

 - на репозиторий с патчами настроен scheduled pipeline: в полночь запускается скрипт, которые анализирует что поменялось относительно предыдущего билда. Если есть изменения - собирает образ(ы) под соотв. платформу, push'ит результат в docker registry и шлет письмо-уведомление на почту;
    
## Недостатки {#docker_minuses}

 - по умолчанию в докере процессы работают с рутовыми правами. Из-за этого возникают проблемы с правами: файлы, сгенеренные рутовым пользователем в контейнере на хосте может менять только рут, неудобно. Решение: при запуске контейнера на лету создается пользователь с id равным id пользователя хоста, который запустил скрипт-обертку;
 - плохая интеграция с IDE, частично решено:
    - запуск компиляция ощуществляется через скрипт-обертку. Вызов этого скрипта легко прописывается в настройки проекта;
    - IDE не имеет доступ к содержимому контейнера, так как контейнер вещь в себе. Потому парсинг проекта не работает - весь SDK с хидерами не доступен. Полукостыльное решение - необходимые хидера просто скопированны на хост. Зачастую этого достаточно; 
 - быстро кушающееся место на диске при достаточно частом обновлении образов, решилось парой команд в скриптах запуска контейнеров:
 
    принудительная очистка висящих слоев, удаление остановленных контейнеров.
    if docker images -f "dangling=true" | grep ago --quiet; then
        docker rmi -f $(docker images -f "dangling=true" -q)
    fi

 - сложно понять различия между двумя образами для одной платформы, но с разными датами сборки. Частично решено это введением паспорта версии, которая ложиться внутрь образа: когда собран, хеши гитов, еще что-то;
 - администратору docker registry неудобно его "администрировать". Нет web ui. Казалось бы типичная задача: просмотреть список доступных репозиториев и образов в них, только через rest api. Или удалить с регистра устаревший образ, дабы освободить место - этого даже в rest api. Без хаков просто не обойтись;
 - у меня не получилось с наскока сделать в Docker Registry разграничение по правам доступа. Типичный сценарий: анонимный pull и авторизированный push. Соответствующее [обсуждение](https://github.com/docker/distribution/issues/1028) на гитхабе. Когда я интересовался вопросом, то было две опции:
    - HTTP basic авторизация - все авторизированные пользователи имеют равные права, не то что надо;
    - авторизация через [nginx](https://docs.docker.com/registry/recipes/nginx/)/[apache](https://docs.docker.com/registry/recipes/apache/), возможно если сделать еще один подход, то будет счастье
 - 

## Заключение   {#resume}

Презенташка от доклада для http://cppusergroup.com.ua:
<iframe width="720" height="405" src="https://docs.google.com/presentation/d/1aBaRGe4nE5KKz8hRUUi9Dsa2Kj3KB6hO84xxJSehCao/embed" frameborder="0"></iframe>

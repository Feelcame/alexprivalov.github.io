---
layout: post
title: "Использование Docker для кросскомпиляции проектов"
permalink: docker-cpp
tags: C++ Docker GitLab
---

<img src='/assets/docker-c++/cplus-docker.jpg' style="width: 720px;">
Docker как незаметный и незаменимый центральный элемент в разработке, Continuous Integration под множество разных целевых платформ.
Как не проходить квест по настройке окружения каждый раз.

---

**Важно**: все нижесказаное описывает мой опыт работы в основном с embedded linux C/C++ проектами. Просьба помнить это во время чтения.

## Содержание
- [Проблема](#problem)
- [Постановка задачи](#formulation)
- [Решение](#solution)
    - [Docker](#docker)
    - [Docker Registry](#registry)
    - [Контейнеры и Continuous Integration(на базе GitLab)](#docker_and_ci)
    - [Минусы и их решение](#docker_minuses)
- [Заключение](#resume)

## Проблема    {#problem}
Сколько времени должен потратить новый сотрудник на то, чтобы собрать и запустить незнакомый ему проект? Обычно это суммарное время, необходимое на следующие действия:

 - выкачать/склонировать репозиторий проекта;
 - настройка окружения разработчика: 
    - установить/настроить средства разработки(тулчейн, IDE, библиотеки);
    - разложить все по правильным путям, сконфигурировать make/cmake/qmake-файл/project_build.sh для сборки;
 - запустить саму сборку;
 - запустить/задеплоить приложение/сервис;

По дороге может выясниться, что где-то чего-то не хватает в зависимостях, неправильно прописан путь к либе/утилите/тулчейну, не та версия библиотеки установлена, где-то не хватает симлинка, где-то лажа с правами. И пока решишь все эти мелкие проблемы - пройдет значительный кусок времени. Плюс еще по любому будут привлекаться сотрудники, которые уже давно работают с проектом. А те в свою очередь со скрипом вспоминают в чем был вопрос, так как проблема была решена полгода назад и о ней уже успели забыть.

Обычно для того что бы каждый раз не набивать шишки - пишется некий README, где прописываются **точные** инструкции, по выполнению которых можно добиться воспроизводимого результата. За пару человек эти инструкции отлаживаются и наступает счастье.

В идеале хочется чтобы сборка проекта выглядела подобным образом:  

{% highlight bash %}
    install all required software, SDKs, libs
    git clone git@project.git
    cd project_dir
    make/cmake/build_project.sh
    deploy
    run
{% endhighlight %}

Какие варианты решений я встречал/проходил:

 - каждый сам себе все настраивает: долго, не воспроизводимо(проблема ["у меня на компьютере все работает"](http://lurkmore.to/%D0%A3%D0%9C%D0%92%D0%A0), так как у каждого разработчика разное окружение), подвержено ошибкам конфигураций. При выпуске новой версии SDK - прохождение квеста каждым разработчиком заново. Автоматизация сборки окружения shell скриптом может частично решить проблему;
 - единое окружение у всех разработчиков, варианты:
    - единожды настроенная виртуальная машина билмастером, все ее себе копируют и работают локально. Новое SDK/библиотека/что-то пропатчено/какое-то важное изменение - новая виртуалка. Тяжело, много места, неудобно;
    - настроенные сервера, у каждого есть свой аккаунт, все туда логинятся по ssh, у каждого своя копия исходников, которая примонтирована на локальную машину разработчика по samba/nfs/sshfs/whatever, и все собираются в уже готовом и настроенном окружении. Сеть/сервер упали - разработчики ковыряются в носу. Качество работы определяется качеством работы сети, в целом невысокая производительность работы IDE(парсинг проекта, подсветка), так как сеть в любом случае проигрывает в скорости доступа локальному диску. Очень осторожно надо менять настройки окружения, ибо если накосячить случайно - зацепит всех, не классно;
    - [linux контейнеры](https://ru.wikipedia.org/wiki/Виртуализация_на_уровне_операционной_системы) - об этом и поговорим далее.

## Постановка задачи    {#formulation}

Представим следующую ситуацию:

 - есть n-ное количество целевых платформ(разные версии linux, gcc, разные процы: [ARM](https://ru.wikipedia.org/wiki/ARM_(%D0%B0%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D0%B0)), [MIPS](https://ru.wikipedia.org/wiki/MIPS_(%D0%B0%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D0%B0)), [SuperH](https://ru.wikipedia.org/wiki/SuperH), x86-64), под который проект собирается, соответственно разные тулчейны для кросскомпиляции;
 - под каждый конкретный процессор идет собственное SDK от чипмейкера. Есть различные версии одного и того же SDK;
 - в SDK находятся баги, их патчат(причем иногда это заметно "сверху", т.е. пользователям SDK, так как могут поменятся интерфейсы), соответственно SDK необходимо пересобрать, бинари/хидера расшарить между разработчиками;
 - то же самое касается еще ряда библиотек, которые работают поверх SDK, например, Qt, Chromium;
 - необходимо свести к минимуму время на развертывание окружения для разработки под конкретную платформу;
 - большинству разработчиков вообще не надо или не интересно заморачиваться настройкой/обновлением окружения, им бы сразу попасть в готовое, где можно писать уже непосредственно код;
 - весь зоопарк платформ надо как-то тестировать, желательно автоматически, хотя бы на предмет успешной компиляции кода проекта под все платформы и при этом не хочется постоянно перенастраивать CI для того, чтобы поддерживать окружения в актуальном состоянии;

Эволюционно решение предстало в виде контейнеризации окружения. Одна платформа - один контейнер со всем необходимым содержимым. Его легко скопировать себе на машину может каждый разработчик и тут же начать работать. Это несомненно проще, чем проходить квест по настройке/обновлению окружения в соответствии с README проекта.

Сразу же на поверхность выплыл еще один вопрос: распостранение контейнеров. Бегать с флешкой/ходить по сети куда-то - не классно, плюс человеческая натура такова, что даже если десять раз скомандовать в рабочем чатике: "обновляемся!", - момент действия будет отложен как можно дальше. Возможность централизированной раздачи контейнеров, да еще если это можно сделать "незаметно" для разработчика - это будет просто песня!

Из всех вариантов(LXC, [OpenVZ](https://openvz.org/), [Docker](https://www.docker.com/) и т.д.), которые были доступны на момент исследований, лишь Docker с его инфраструктурой подошел целиком и полностью.

## Решение  {#solution}

### Docker  {#docker}

Цитата с [вики](https://ru.wikipedia.org/wiki/Docker):

    Docker — программное обеспечение для автоматизации развёртывания и управления приложениями в среде виртуализации на уровне операционной системы. Позволяет «упаковать» приложение со всем его окружением и зависимостями в контейнер, который может быть перенесён на любую Linux-систему с поддержкой cgroups в ядре, а также предоставляет среду по управлению контейнерами.

Как было до внедрения docker'a: были сервера, где располагались настроенные/собранные sdk/библиотеки. Все заинтересованные разработчики логинились туда по ssh, монтировали с сервера на свой локальный хост по nfs/samba/sshfs папки с проектами, открывали их в локальном любимом редакторе. На локальном хосте редактировали, на удаленном сервере - компилировали. Это работало: у всех единное окружение, настраивать его всем каждый раз не надо. Но медленно: проекты большие, IDE постоянно подвисает на сетевых операциях, на сервере всегда кто-то компилирует - ты ждешь пока скомпилятся твои изменения, в то время как твой хост с 4хядерным Intel Core i7 просто простаивает, печаль:(.

После введения контейнеров, как у нас в компании получилось:

 - каждый разработчик работает локально, не зависит от сети и ее лагов;
 - под одну платформу есть две "последние" версии docker-образа: latest и latest_debug. Первая версия - это почищенная вторая от временных файлов(обьектники, архивы, сырцы либ, .git-папки и все то что точно не потребуется для разработки приложений просто использующих sdk/библиотеки), соответственно latest_debug - это просто дамп всего того что нагенерировалось во время сборки sdk и библиотек;
 - разработчики приложений - используют готовое запакованное в latest docker-образ окружение для сборки проектов;
 - разработчики платформы - пилят и патчат саму программную "платформу": то что используется разработчиками приложений. Т.е. linux, SDK от чипмейкера, Qt, Webkit\Chromium. Для этого используют latest_debug версию контейнера. Не страшно накосячить и что-то не так сделать - в крайнем случае все просто потеряется и откатится к первоначальному состоянию. Потому можно смело эксперементировать, не боясь "сломать" достаточно сложную многоступенчатую сборку "мира", которая уже настроена. Опять же, удобно использовать latest_debug версию из-за того что уже есть все в собранном состоянии, при изменениях компилируется только дельта. Т.е. не приходится ждать пока собирется, например, весь Qt5 Framework(который еще надо и корректно сконфигурировать для кросскомпиляции!);
 - latest docker-образы используются на CI. Всегда есть платформы в состоянии активной разработки и те, которые сейчас на поддержке: "туда разработчики редко заходят". До введения CI часто возникали ситуации, когда билд был сломан для редкоразрабатываемой на данный момент платформы. И прежде чем приступить к работе на ней, необходимо было в обязательном порядке фиксить ошибки. CI просто исключил такие ситуации - виновник <s>торжества</s> поломки сразу получает письмо от GitLab и бежит исправлять ошибки;
 - введен в строй приватный [docker registry](#registry) - сервер хранения образов - все заинтересованные получают образы только с этого сервера. Туда же выкладываются обновленые версии образов;

Как запускается докер образ

Картинка: что происходит по push'у разработчика в репозиторий проекта(как CI работает и взаимодействует с docker registry)
![](/assets/docker-c++/slide1.png)


### Docker Registry {#registry}

С офф. [сайта](https://docs.docker.com/registry/):

    The Registry is a stateless, highly scalable server side application that stores and lets you distribute Docker images. The Registry is open-source, under the permissive Apache license.
    
В общем-то все сказано:). Централизованное хранение и раздача docker-образов. Можно скидывать образы на какой-то ftp/web-сервер, облако. При таком варианте каждый разработчик должен сам сознательно скачать на свою машину необходимый образ; что-то вроде следующих действий:

    wget https://cloud.company.com/path/to/dockers/specific_img_version.tar
    docker load specific_img_version.tar
    rm -f specific_img_version.tar

Где specific_img_version.tar - название экспортированного docker образа.

А можно поднять свой Docker Registry, откуда сам docker клиент будет тянуть требуемый образ.

В общем случае мне кажется логичным использовать для этих целей [Docker Hub](hub.docker.com) - аналог Github в мире docker контейнеров. Там можно размещать неограниченное количество публичных образов и только один приватный. Большее количество приватных образов можно добавлять за отдельную плату. Если нет ничего секретного - заводим аккаунт, выкладываем образы, которые публичны и доступны всем. Если хотим "приватности" - покупается [подписка](https://hub.docker.com/billing-plans/) и можно размещать приватные образа, т.е. публично не доступные. Эти два варианты клевые тем, что сразу снимается вопрос поддержки - за нас это делают админы Docker Hub. В нашем случае это не подходило(политика компания, есть третьестороние компоненты, которые мы не имеем права хранить за пределами компании), потому подняли собственный docker registry. Он не такой крутой как Docker Hub. Нет, например, какого либо веб интефейса. "Общаться" с сервером можно через rest api. Например, вот как получить список репозиториев при помощи curl'a:
    
    curl -X GET https://hub.docker.company_name.com.ua:5000/v2/_catalog - общий список репозиториев
    curl -X GET https://hub.docker.company_name.com.ua:5000/v2/repo1/mag324/tags/list - список образов для репозитория repo1
    curl -X GET https://hub.docker.company_name.com.ua:5000/v2/repo2/mag351/tags/list - список образов для репозитория repo2

Сам сервер поставляется в виде, как не странно, docker-образа и запускается следующим образом:

     docker run -d -p 5000:5000 --restart=always --name registry -v /docker_registry:/var/lib/registry -v /docker_registry/certs:/certs -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/fullchain.pem -e REGISTRY_HTTP_TLS_KEY=/certs/privkey.pem registry:2
     
где:

 - **\-\-restart=always** - всегда перезапускать контейнер;
 - **/docker_registry** - папка на сервере, которая будет хранить все выгруженные в регистр образы, "прокидывается" внутрь контейнера по пути /var/lib/registry (констукция **-v /path_on_host:/path_inside_container**);
 - **REGISTRY_HTTP_TLS_CERTIFICATE**, **REGISTRY_HTTP_TLS_KEY** - переменные окружения в контейнере, которые содержат пути к сертификату и ключу;
 - более подробное [руководство](https://docs.docker.com/registry/deploying/) по регистру:)
 
    
После этого у вас есть настроенный и готовый для работы регистр.

**Важное** примечание: docker client по умолчанию работает через https. Если у вас нет/не планируется SSL для домена, на котором будет крутиться регистр, то на каждом хосте, где будет запускаться docker client, необходимо [прописать](https://docs.docker.com/registry/insecure/#deploy-a-plain-http-registry) адрес "небезопасного" docker registry в /etc/docker/daemon.json:

    {
      "insecure-registries" : ["registry_ip_without_ssl.com:5000"]
    }

По умолчанию docker client всегда "смотрит" на Docker Hub. Для того что бы он мог пушить и пулить образы с других хранилищ, в имя образа всегда надо добавлять адрес docker registry. Пример имени, который используется у нас в компании:

    hub.company.com:5000/chipmaker_name/platform_name:latest

где hub.company.com:5000 - url и порт, на котором работает docker regsitry, если эта часть будет пустая docker client будет посылать запросы на Docker Hub.
Вся дальнейшая часть имени может быть абсолютно произвольной.

Отправить образ в хранилище:

    docker push registry_ip:registry_port/image_name:image_version

Получить образ из хранилища:

    docker pull registry_ip:registry_port/image_name:image_version

В общем-то это все команды, которые используются у нас в компании для взаимодействия между docker client и docker registry.


Схема работы такова:

 - на рабочем компьютере билдмастера(либо на CI) собирается образ, который содержит все необходимое для разработки под конкретную платформу, проверяется его валидность и отправляется в регистр:
{% highlight bash %}
    docker push registry_ip:registry_port/image_name:image_version
{% endhighlight %}
 - все желающие забирают/пулят образ из регистра и работают в готовом окружении:
{% highlight bash %}
    docker pull registry_ip:registry_port/image_name:image_version
{% endhighlight %}
 - удаляют локальную копию предыдущей версии образа, **по хешу(IMAGE ID), а не по имени**:
{% highlight bash %}
    docker rmi e90ae3da554e
{% endhighlight %}

Почему по хешу будет понятно позже.

Запуск:
{% highlight bash %}
    docker run -it --rm -v /path/to/project/sources:/path/of/sources/inside/container registry_ip:registry_port/image_name:image_version bash
{% endhighlight %}
В результате получаем обычную командную строку(в данном случае bash), в которой можно перейти в папку с проектом и вызвать make/cmake/build.sh
Можно запустить сразу билд, что бы ручками не ходить куда-то:

{% highlight bash %}
    docker run -it --rm -v /path/to/project/sources:/path/of/sources/inside/container registry_ip:registry_port/image_name:image_version bash -c "cd /path/of/sources/inside/container; make/cmake/build.sh;"
{% endhighlight %}
Команду выше можно использовать для интеграции с IDE: в каждой приличной IDE есть что-то вроде custom build steps, куда приведенную команду можно прописать.

Все вышеуказанные команды имеют право на жизнь и использование, но ... большинству разработчиков они не нужны/интересны. Потому вокруг докера была написана простая обертка на bash.

## Скрипт-обертка

 - одна и та же везде: и для разработчиков и для CI;
 - лежит в гите вместе с проектом;
 - решает следующие задачи:
    - нулевое вхождение: не надо изучать docker чтобы пользоваться;
    - "прокидывает" папку с проектом внутрь контейнера;
    - создает "на лету" пользователя аналогичного хостовому - нет проблем с правами на сгенерированные файлы(те, которые получились в результате компиляции, так как в контейнере по умолчанию процессы работают от рута);
    - настраивает корректную работу git внутри контейнера;
    - проверяет на наличие и удаляет "висящие" слои/образы, остановленные контейнеры - место на диске не пропадает впустую;
    - кого не устраивает дефолтная функциональность - используют докер напрямую с его километровыми командами:)

скрипт запуска:
{% highlight bash %}
#!/bin/bash

DOCKER_IMG_NAME="docker_image_name:latest"

set -e

docker pull ${DOCKER_IMG_NAME}

self_script_name="$(basename "$(test -L "$0" && readlink "$0" || echo "$0")")"

src_dir=${1?Usage: ${self_script_name} path_to_sources}
resolved_dir=$(readlink -f $src_dir)

echo "Will mount host path \"${resolved_dir}\" to container path \"/src\""

hint_msg=$(cat <<EOF
#################################################
#Some infomation tips                           #
#################################################
EOF
)
bash_cmd=$(cat <<EOF
echo '${hint_msg}' && bash
EOF
)

docker run -it --rm -v ${resolved_dir}:/src -e LOCAL_USER_ID=`id -u ${USER}` ${DOCKER_IMG_NAME} bash -c "${bash_cmd}"
{% endhighlight %}

Как используем скрипт:
{% highlight bash %}
cd project_dir
./dockerBuild.sh platformName mode path_to_project optional_cmd
{% endhighlight %}
где:

 - dockerBuild.sh - скрипт-обертка над докером;
 - platformName - имя платформы, под которую необходимо собрать проект;
 - mode - режим работы; их всего два: dev и CI. Первый - режим разработчика: получаем bash c [cwd](https://en.wikipedia.org/wiki/Working_directory) в папке с проектом, где можно собрать проект, работать с гитом и т.п. Второй режим - запускает автоматически сборку, используется на CI.
 - optional_cmd - необязательный параметр, имеет смысл только в dev-режиме: команда, которую выполнить в контейнере и выйти.

## Автоматическая сборка образов

 - есть репозиторий, который содержит [dockerfile](https://docs.docker.com/engine/reference/builder/)'ы, bash-скрипты, конфиги для сборки docker-образа под ту или иную платформу;
 - есть репозиторий с патчами для SDK, структура приблизительно следующая:
 
{% highlight bash %}
    Patches_repo  
        |-->Platform1  
        |       |-->SDKv1  
        |       |     |-->01_patch_name.patch  
        |       |     |-->02_patch_name.patch  
        |       |     |-->.....  
        |       |     |-->nn_patch_name.patch  
        |       |-->SDKv1.5  
        |             |-->01_patch_name.patch  
        |             |-->02_patch_name.patch  
        |             |-->.....  
        |             |-->nn_patch_name.patch  
        |-->Platform2  
        |       |-->SDKv1  
        |       |     |-->...  
        |       |-->SDKv1.7  
        |             |-->...  
        |-->....  
        |-->PlatformN  
                |-->...  
{% endhighlight %}
 
 - есть сами оригинальные SDK от чипмейкеров;
 - любые изменения в репозитории с патчами - сигнал к тому что необходимо обновить образ - у всех должно быть последнее актуальное SDK;
 - на push в репозиторий с патчами настроен [pipeline](https://docs.gitlab.com/ee/ci/pipelines.html), который каждый раз запускает утилиту patch в контейнере с чистыми sdk - успешно ли все патчится? не ноль в качестве возврата программы patch - тот кто сделал изменения получает письмо и должен исправить ошибку;
 - на репозиторий с патчами настроен [scheduled pipeline](https://docs.gitlab.com/ce/user/project/pipelines/schedules.html): в полночь запускается скрипт, который анализирует что поменялось относительно предыдущего билда. Если есть изменения - собирает образ(ы) под соответсвующую платформу, валидирует образ(успешно ли собирается проект в контейнере? запускается ли он на таргете?), push'ит результат в docker registry и шлет письмо-уведомление на почту;
    
![](/assets/docker-c++/slide2.png)
<span class="signed-image">Как собирается docker образ</span>
    
## Минусы и их решение {#docker_minuses}

 - по умолчанию в докер-контейнере процессы работают с рутовыми правами. Из-за этого возникают проблемы с правами: файлы, сгенеренные рутовым пользователем в контейнере на хосте может менять только рут, неудобно. Решение: при запуске контейнера на лету создается пользователь с id равным id пользователя хоста, который запустил скрипт-обертку;
 - плохая интеграция с IDE, частично решено:
    - запуск компиляция ощуществляется через скрипт-обертку. Вызов этого скрипта легко прописывается в настройки проекта;
    - IDE не имеет доступ к содержимому контейнера, так как контейнер вещь в себе. Потому парсинг проекта не работает - весь SDK с хидерами не доступен. Полукостыльное решение - необходимые хидера просто скопированны на хост. Зачастую этого достаточно; 
 - быстро кушающееся место на диске при достаточно частом обновлении образов, решилось парой команд в скриптах запуска контейнеров:

{% highlight bash %}
    принудительная очистка висящих слоев, удаление остановленных контейнеров.
    if docker images -f "dangling=true" | grep ago --quiet; then
        docker rmi -f $(docker images -f "dangling=true" -q)
    fi
{% endhighlight %}

 - сложно понять различия между двумя образами для одной платформы, но с разными датами сборки. Частично решено это введением паспорта версии, которая ложиться внутрь образа: когда собран, хеши гитов, еще что-то;
 - администратору docker registry неудобно его "администрировать". Нет web ui. Казалось бы типичная задача: просмотреть список доступных репозиториев и образов в них, только через rest api. Или удалить с регистра устаревший образ, дабы освободить место - этого даже в rest api. Без хаков просто не обойтись;
 - у меня не получилось с наскока сделать в Docker Registry разграничение по правам доступа. Типичный сценарий: анонимный pull и авторизированный push. Соответствующее [обсуждение](https://github.com/docker/distribution/issues/1028) на гитхабе. Когда я интересовался вопросом, то было две опции:
    - HTTP basic авторизация - все авторизированные пользователи имеют равные права, не то что надо;
    - авторизация через [nginx](https://docs.docker.com/registry/recipes/nginx/)/[apache](https://docs.docker.com/registry/recipes/apache/), возможно если сделать еще один подход, то будет счастье
